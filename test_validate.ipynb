{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.special\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.base import clone\n",
    "# import joblib\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "\n",
    "from flaml import AutoML\n",
    "from flaml import AutoML\n",
    "\n",
    "from myflaml import auto_reg, auto_clf, auto_weighted_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "time_budget = 60 # time budget for auto-ml in seconds (advisable at least 120)\n",
    "verbose = 0 # verbosity of auto-ml\n",
    "n_splits = 5 # cross-fitting and cross-validation splits\n",
    "data = '401k' # which dataset, one of {'401k', 'criteo', 'welfare', 'poverty', 'star'}\n",
    "plot = True # whether to plot results\n",
    "xfeat = 'inc' # feature to use as x axis in plotting, e.g. for criteo 'f1', for 401k 'inc', for welfare 'polviews'\n",
    "# Formula for the BLP of CATE regression.\n",
    "blp_formula = 'np.log(inc)' # e.g. 'f1' for criteo, np.log(inc)' for 401k, 'C(polviews)' for the welfare case.\n",
    "hetero_feats = ['inc'] # list of subset of features to be used for CATE model or the string 'all' for everything\n",
    "binary_y = False\n",
    "\n",
    "## For semi-synthetic data generation\n",
    "semi_synth = False # Whether true outcome y should be replaced by a fake outcome from a known CEF\n",
    "simple_synth = True # Whether the true CEF of the fake y should be simple or fitted from data\n",
    "max_depth = 2 # max depth of random forest during for semi-synthetic model fitting\n",
    "scale = .2 # magnitude of noise in semi-synthetic data\n",
    "def simple_true_cef(D, X): # simple CEF of the outcome for semi-synthetic data\n",
    "    return .5 * np.array(X)[:, 1] * D + np.array(X)[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from datasets import fetch_data_generator\n",
    "\n",
    "get_data, abtest, true_cef, true_cate = fetch_data_generator(data=data, semi_synth=semi_synth,\n",
    "                                                             simple_synth=simple_synth,\n",
    "                                                             scale=scale, true_f=simple_true_cef,\n",
    "                                                             max_depth=max_depth)\n",
    "X, D, y, groups = get_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "if groups is None:\n",
    "    X, Xval, D, Dval, y, yval = train_test_split(X, D, y, train_size=.6, shuffle=True, random_state=123)\n",
    "    Xval, Xtest, Dval, Dtest, yval, ytest = train_test_split(Xval, Dval, yval, train_size=.5, shuffle=True, random_state=123)\n",
    "    groupsval, groupstest = None, None\n",
    "else:\n",
    "    train, val = next(GroupShuffleSplit(n_splits=2, train_size=.6, random_state=123).split(X, y, groups=groups))\n",
    "    X, Xval, D, Dval, y, yval = X.iloc[train], X.iloc[val], D[train], D[val], y[train], y[val]\n",
    "    groups, groupsval = groups[train], groups[val]\n",
    "\n",
    "    val, test = next(GroupShuffleSplit(n_splits=2, train_size=.5, random_state=123).split(Xval, yval, groups=groupsval))\n",
    "    Xval, Xtest, Dval, Dtest, yval, ytest = Xval.iloc[val], Xval.iloc[test], Dval[val], Dval[test], yval[val], yval[test]\n",
    "    groupsval, groupstest = groupsval[val], groupsval[test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "mreg, my, mt, mreg_zero, mreg_one = joblib.load('nuisance.jbl')\n",
    "model_reg = lambda: clone(mreg)\n",
    "model_y = lambda: clone(my)\n",
    "model_t = lambda: clone(mt)\n",
    "model_reg_zero = lambda: clone(mreg_zero)\n",
    "model_reg_one = lambda: clone(mreg_one)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if groups is None:\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "    splits = list(cv.split(X, D))\n",
    "else:\n",
    "    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "    splits = list(cv.split(X, D, groups=groups))\n",
    "\n",
    "n = X.shape[0]\n",
    "reg_preds = np.zeros(n)\n",
    "reg_zero_preds = np.zeros(n)\n",
    "reg_one_preds = np.zeros(n)\n",
    "reg_preds_t = np.zeros(n)\n",
    "reg_zero_preds_t = np.zeros(n)\n",
    "reg_one_preds_t = np.zeros(n)\n",
    "\n",
    "DX = np.column_stack((D, X))\n",
    "for train, test in splits:\n",
    "    reg = model_reg().fit(DX[train], y[train])\n",
    "    reg_preds[test] = reg.predict(DX[test])\n",
    "    reg_one_preds[test] = reg.predict(np.column_stack([np.ones(len(test)), X.iloc[test]]))\n",
    "    reg_zero_preds[test] = reg.predict(np.column_stack([np.zeros(len(test)), X.iloc[test]]))\n",
    "\n",
    "    reg_zero = model_reg_zero().fit(X.iloc[train][D[train]==0], y[train][D[train]==0])\n",
    "    reg_one = model_reg_one().fit(X.iloc[train][D[train]==1], y[train][D[train]==1])\n",
    "    reg_zero_preds_t[test] = reg_zero.predict(X.iloc[test])\n",
    "    reg_one_preds_t[test] = reg_one.predict(X.iloc[test])\n",
    "    reg_preds_t[test] = reg_zero_preds_t[test] * (1 - D[test]) + reg_one_preds_t[test] * D[test]\n",
    "\n",
    "res_preds = cross_val_predict(model_y(), X, y, cv=splits)\n",
    "prop_preds = cross_val_predict(model_t(), X, D, cv=splits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if hetero_feats == 'all':\n",
    "    hetero_feats = X.columns\n",
    "Z, Zval, Ztest = X[hetero_feats], Xval[hetero_feats], Xtest[hetero_feats]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "if groups is None:\n",
    "    split_type = 'auto'\n",
    "else:\n",
    "    split_type = GroupKFold(n_splits=n_splits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "slearner_best = joblib.load('slearner.jbl')[0]\n",
    "slearner = slearner_best.fit(Z, reg_one_preds - reg_zero_preds)\n",
    "cate_model = slearner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class DRLinear:\n",
    "    \"\"\"\n",
    "    x = DRLinear(cate, zero, one, t)\n",
    "    x_fitted = x.fit(X,Y,D,Z)\n",
    "\n",
    "    x.model ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cate_model,\n",
    "        model_y_zero,\n",
    "        model_y_one,\n",
    "        model_t,\n",
    "    ):\n",
    "        self.cate_model = cate_model\n",
    "        self.model_y_zero = model_y_zero\n",
    "        self.model_y_one = model_y_one\n",
    "        self.model_t = model_t\n",
    "\n",
    "    def calculate_dr_outcomes(\n",
    "        self,\n",
    "        X,\n",
    "        D,\n",
    "        y\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: covariate data\n",
    "        :param D: treatment assignment\n",
    "        :param y: outcomes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        reg_zero_preds_t = self.model_y_zero.predict(X)\n",
    "        reg_one_preds_t = self.model_y_one.predict(X)\n",
    "        reg_preds_t = reg_zero_preds_t * (1 - D) + reg_one_preds_t * D\n",
    "        prop_preds = self.model_t.predict(X)\n",
    "\n",
    "        dr = reg_one_preds_t - reg_zero_preds_t\n",
    "        reisz = (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), .09, np.inf)\n",
    "        dr += (y - reg_preds_t) * reisz\n",
    "\n",
    "        return dr\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        D,\n",
    "        Y,\n",
    "        Z\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: covariate data\n",
    "        :param D: treatment assignment\n",
    "        :param y: outcomes\n",
    "        :param Z: subsetted covariates on which to test heterogeneity\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.dr_outcomes_ = self.calculate_dr_outcomes(X, D, Y)\n",
    "\n",
    "        self.cate_predictions_ = self.cate_model.predict(Z)\n",
    "\n",
    "        self.model = OLS(self.dr_outcomes_, add_constant(self.cate_predictions_)).fit()\n",
    "\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "reg_zero = model_reg_zero().fit(X[D==0], y[D==0])\n",
    "reg_one = model_reg_one().fit(X[D==1], y[D==1])\n",
    "reg_t = model_t().fit(X, D)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.005\nModel:                            OLS   Adj. R-squared:                  0.004\nMethod:                 Least Squares   F-statistic:                     9.563\nDate:                Wed, 26 Apr 2023   Prob (F-statistic):            0.00201\nTime:                        14:19:10   Log-Likelihood:                -25255.\nNo. Observations:                1944   AIC:                         5.051e+04\nDf Residuals:                    1942   BIC:                         5.052e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst      -7612.4087   6502.176     -1.171      0.242   -2.04e+04    5139.570\nx1             2.8019      0.906      3.092      0.002       1.025       4.579\n==============================================================================\nOmnibus:                     3130.243   Durbin-Watson:                   2.018\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3363335.232\nSkew:                           9.945   Prob(JB):                         0.00\nKurtosis:                     205.798   Cond. No.                     1.94e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.94e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.005</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.563</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 26 Apr 2023</td> <th>  Prob (F-statistic):</th>  <td>0.00201</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:19:10</td>     <th>  Log-Likelihood:    </th> <td> -25255.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  1944</td>      <th>  AIC:               </th> <td>5.051e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  1942</td>      <th>  BIC:               </th> <td>5.052e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>-7612.4087</td> <td> 6502.176</td> <td>   -1.171</td> <td> 0.242</td> <td>-2.04e+04</td> <td> 5139.570</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    2.8019</td> <td>    0.906</td> <td>    3.092</td> <td> 0.002</td> <td>    1.025</td> <td>    4.579</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>3130.243</td> <th>  Durbin-Watson:     </th>  <td>   2.018</td>  \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3363335.232</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 9.945</td>  <th>  Prob(JB):          </th>  <td>    0.00</td>  \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>205.798</td> <th>  Cond. No.          </th>  <td>1.94e+04</td>  \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.94e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model = DRLinear(slearner, reg_zero, reg_one, reg_t)\n",
    "fitted = val_model.fit(Xtest, Dtest, ytest, Ztest)\n",
    "fitted.model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calibration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class cal_scorer:\n",
    "    \"\"\"\n",
    "    x = DRLinear(cate, zero, one, t)\n",
    "    x_fitted = x.fit(X,Y,D,Z)\n",
    "\n",
    "    x.model ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cate_model,\n",
    "        model_y_zero,\n",
    "        model_y_one,\n",
    "        model_t,\n",
    "        n_groups\n",
    "    ):\n",
    "        self.cate_model = cate_model\n",
    "        self.model_y_zero = model_y_zero\n",
    "        self.model_y_one = model_y_one\n",
    "        self.model_t = model_t\n",
    "        self.n_groups = n_groups\n",
    "\n",
    "    def calculate_dr_outcomes(\n",
    "        self,\n",
    "        X,\n",
    "        D,\n",
    "        y\n",
    "    ):\n",
    "        reg_zero_preds= self.model_y_zero.predict(X)\n",
    "        reg_one_preds = self.model_y_one.predict(X)\n",
    "        reg_preds = reg_zero_preds * (1 - D) + reg_one_preds * D\n",
    "        prop_preds = self.model_t.predict(X)\n",
    "\n",
    "        dr = reg_one_preds - reg_zero_preds\n",
    "        reisz = (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), .01, np.inf)\n",
    "        dr += (y - reg_preds) * reisz\n",
    "\n",
    "        return dr\n",
    "\n",
    "    def score(\n",
    "        self,\n",
    "        Xval,\n",
    "        Dval,\n",
    "        Yval,\n",
    "        Zval,\n",
    "        Ztest\n",
    "    ):\n",
    "\n",
    "        self.dr_outcomes_val_ = self.calculate_dr_outcomes(Xval, Dval, Yval)\n",
    "\n",
    "        self.cate_preds_val_ = self.cate_model.predict(Zval)\n",
    "        self.cate_preds_test_ = self.cate_model.predict(Ztest)\n",
    "\n",
    "        probs = np.zeros(self.n_groups)\n",
    "        g_cate = np.zeros(self.n_groups)\n",
    "        gate = np.zeros(self.n_groups)\n",
    "\n",
    "        cuts = np.quantile(self.cate_preds_test_, np.linspace(0, 1, self.n_groups + 1))\n",
    "        for i in range(self.n_groups):\n",
    "            ind = (self.cate_preds_val_ >= cuts[i]) & (self.cate_preds_val_ < cuts[i + 1])\n",
    "            probs[i] = np.mean(ind)\n",
    "            gate[i] = np.mean(self.dr_outcomes_val_[ind])\n",
    "            g_cate[i] = np.mean(self.cate_preds_val_[ind])\n",
    "\n",
    "        ate = np.mean(self.dr_outcomes_val_)\n",
    "\n",
    "        diff1 = np.sum(abs(gate - g_cate) * probs)\n",
    "\n",
    "        diff2 = np.sum(abs(gate - ate) * probs)\n",
    "\n",
    "        self.cal_score = 1 - (diff1 / diff2)\n",
    "\n",
    "        self.gate = gate\n",
    "        self.g_cate = g_cate\n",
    "        self.probs = probs\n",
    "\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "cal_model = cal_scorer(slearner, reg_zero, reg_one, reg_t, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.12329395471163718"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted = cal_model.score(Xval, Dval, yval, Zval, Ztest)\n",
    "fitted.cal_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6511.78222656, -2845.09619141,  6624.68408203,  5454.87402344])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.gate\n",
    "fitted.gate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}